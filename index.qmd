---
title: "Optimizing Agentic Retrieval-Augmented Generation: An Analysis of Single vs. Multi-Agent Workflows"
author: "Cheyanne Allred-Lopez (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

## Introduction

Universities are increasingly offering more diverse courses and specialized degree programs, allowing students to tailor their coursework to their interests or career goals. This subsequently increases the complexity and specificity of students' advising needs, increasing the load on the university's academic advisers. Advising demand peaks during specific periods, including  registration, drop/add periods, and in the days following final grade issuance. This creates a bottleneck, as there are a fixed number of advisers available to address an increasing number of students' needs. While many universities publish academic policies and degree requirements online, students may wish to verify the applicability of the information to their specific academic situation.

While chatbots have long existed to help serve customers' requests, their performance has been limited by reliance on static information, the inability to synthesize information across many sources or apply context-specific reasoning, as well as the inability to generate natural responses. Recent advancements in large language models, including the ability to dynamically access and synthesize information across multiple sources, understand context and autonomously adjust behavior, and generate conversational responses, present an opportunity to develop an intelligent academic advising assistant. This assistant could augment existing advisers by responding to routine questions and providing personalized guidance, consequently allowing advisers to focus on higher-complexity requests.

Off-the-shelf large language models have increasingly large parametric knowledge bases, though their knowledge is limited to that which they have been exposed to during training. The models do not have access to any information published after their training cutoff date. Despite this, models may attempt to answer a question about information that they were not trained upon, resulting in hallucinated responses. Furthermore, generated responses do not have source information, leaving users unable to verify the accuracy of responses. These limitations are unacceptable in an academic setting. Academic policies, degree requirements, and program offerings change regularly. An intelligent advising assistant must be able to access the most up-to-date information at all times, providing grounded, verifiable responses with clear citations. 

Retrieval-augmented generation (RAG) addresses these limitations by enabling access to non-parametric knowledge, or information retrieved from external information stores at the time of the user's query rather than from within the model's parametric knowledge [@lewis2020retrieval]. This expands the model's memory and reduces hallucinations, improving the model's performance. The architecture combines a pre-trained retrieval model, which includes a query encoder and a document index, with a pre-trained generation model. The process is as follows: The retrieval model encodes the user's query and uses Maximum Inner Product Search (MIPS) to identify the top-K most relevant documents from the index. The generation model then crafts its response based on both the user's original query and the retrieved documents. The RAG architecture outperforms state-of-the-art models on knowledge-intensive tasks. @lewis2020retrieval also demonstrate the advantages of combining retrieval with generation, which increases the overall performance by supplementing the generation model's parametric knowledge. Additionally, human evaluators preferred the responses generated by a RAG architecture over responses from generation-models, emphasizing the importance of grounding the model's responses. This architecture overcomes the knowledge-cutoff date problem present in large language models. The model's non-parametric knowledge can be updated simply by encoding and upserting the updated information to the document index. This is a more practical option, as it is computationally less expensive, less time-consuming, and less technically complex than retraining the generation model. Additionally, this architecture provides transparency in terms of information sources. Whereas parametric knowledge is stored within the model's training weights, information obtained using a retriever has a citation, improving the verifiability of responses. Since 2020, the RAG architecture has evolved significantly. @gao2024rag systematically reviewed over 100 RAG-related studies, identifying three technological paradigms: Naive RAG, Advanced RAG, and modular RAG. They document the numerous optimization techniques that can improve RAG system performance. This review focuses on techniques relevant to the development of an agentic academic adviser.

Naive RAG is identical to the architecture championed by @lewis2020retrieval. @gao2024rag note that while naive RAG's performance is high compared to non-RAG enabled LLMs, naive RAG systems have three key limitations: retrieval of irrelevant documents, reliance on parametric memory leading to hallucinated answers, and inefficient information synthesis across multiple sources. Advanced RAG implements both pre-retrieval and post-retrieval optimization techniques to overcome the shortcomings of naive RAG. Pre-retrieval optimizations include query optimization strategies, document-index improvements, and embedding optimizations. Query optimization strategies are employed to refine the user's intent before retrieval and include techniques such as utilizing an LLM to break down, expand, or rewrite a user's query. Indexing optimization focuses on how data is structured. Strategies include recursively splitting a document using its structure and overlapping context between chunks, preventing context truncation. Additionally, indexes can utilize hierarchical structures, knowledge graphs, or embedded metadata to capture relationships between data chunks, increasing context to improve the relevancy of retrieved information. The authors' comprehensive review identifies multiple embedding optimization techniques, including hybrid indexing and embedding model fine-tuning. Fine-tuning a model can be computationally expensive and technically rigorous, limiting its applicability. Hybrid indexes, however, are a more accessible alternative, utilizing sparse and dense vector representations to capture both contextual information and exact keyword matching.

@BlendedRAG provide empirical evidence of hybrid indexes' superior performance when compared to state-of-the-art models. The authors explore indexing techniques to improve the retrieval effectiveness of RAG systems, including standard sparse indexing, dense indexing, and sparse encoder-based vector models. Standard sparse indexing, colloquially known as keyword-matching or lexical search, assigns relevance based upon how many times a keyword appears in a document relative to the collection of documents. Dense indexes work by comparing the vectorized form of the user’s query to a collection of vectorized documents, selecting the top k documents using cosine similarity. This method is efficient for capturing context. Sparse-encoder indexes expand upon typical sparse indexes by utilizing a model to expand both the user’s query and the collection of documents with related words before searching, increasing the index size and improving precision. Benchmarking using the NDCG@10 metric, the authors demonstrated that semantic-based hybrid methods consistently outperform both state-of-the-art baselines and keyword-based hybrid methods. The Sparse Encoder-based semantic search, when combined with the 'Best Fields' query, demonstrated superior performance, even when compared to RAG pipelines specifically fine-tuned for those datasets. These findings are particularly relevant in an academic advising context, where users' queries may require contextual understanding, benefiting from dense embeddings, or be better suited by exact keyword-matching, benefiting from sparse embeddings. 

Following retrieval, Advanced RAG also implements post-retrieval optimization techniques, which refine the context prior to generation. In the generation process, the LLM synthesizes the retrieved results into an answer which is returned to the user. @gao2024rag highlight that it is generally not a good idea to return the results as-is and recommend curating the context before returning the information to the user. This may involve re-ranking outputs to ensure that the most relevant answer is provided first. Other post-retrieval techniques include context compression, using a small language model (SLM) to pre-filter noise, or using an LLM to evaluate the content prior to returning an answer to the user.


@gao2024rag identify that robust RAG systems should be able to reject documents which are contextually-relevant but non-substantive, as well as refrain from generating an answer when the returned results are not useful, thus minimizing hallucinations. RAG systems should also be able to synthesize information across multiple documents and identify known inaccuracies. To ensure systems possess these capabilities, @gao2024rag emphasize that RAG systems inherently have two objectives, retrieval and generation, which should be evaluated separately. The retrieved results should be evaluated for precision and recall, while the generated text should be evaluated against the context of what was returned (faithfulness) and its relevancy to the initial query. This enables optimization of specific pipeline components. 

Despite the importance of evaluating the components individually, RAG systems have traditionally been evaluated end-to-end, combining retrieval and generation performance into a single evaluation. To address this limitation, @salemi2024eRAG propose eRAG, a method for evaluating retrieval quality independently. eRAG is a system where the LLM individually processes each document returned by the retriever and generates an answer. Metrics, such as Exact Match or ROUGE, can be employed to generate a utility score based upon the LLM’s ability to answer the user’s query using only that document’s content. The accuracy scores can then be aggregated set-wise, and the retrieved result set can be evaluated using standard metrics, including precision and recall. The authors demonstrate that this method of evaluation is more computationally efficient, reducing runtime complexity from $O(lk^2d^2)$ to $O(lkd^2)$, where k is the number of documents, d is the average length of each document, and l is the length of the generated output. Standard transformers apply a runtime penalty to long inputs, scaling the cost quadratically. Running the LLM k times, rather than combining all k documents into a single input, helps to avoid this quadratic cost penalty.

Upon evaluation, the authors found that eRAG consistently outperformed more traditional systems, including the KILT benchmark and Relevance Annotation with LLM. The eRAG method also outperformed all other methods as the number of documents retrieved increased. Finally, the authors compared eRAG evaluation time and memory consumption to that of traditional end-to-end evaluation methods, finding that, on average, eRAG is 2.5 times faster and significantly more memory-efficient, depending on whether the evaluation process is being performed at a document-level or query-level. eRAG is directly applicable to this research during both system development and in production. During development, eRAG is an efficient way to compare retriever performance as different optimization techniques are implemented. It can also be used in production as a way to filter returned text chunks for relevancy and quality, before generating a response to the user.

Naive and advanced RAG pipelines are linear in flow: The system receives the user's query, retrieves the relevant documents, returns the relevant context to the LLM, which generates a result. Modular RAG instead provides the capability to alter this flow, building upon the foundational principles established by both Naive RAG and Advanced RAG. Modular RAG systems aim to improve context, flexibility, and overall efficiency. Augmentation techniques, in the form of advanced architectures, capitalize on this by enabling loops. Iterative retrieval initializes with the usual linear process. The generated response is then appended to the initial user query and re-sent through the pipeline, enhancing the context. This process repeats until the number of iterations exceeds a pre-defined threshold. Recursive retrieval also initializes with a complete cycle; however, it then allows the generator LLM to judge the relevance of the most recent results. The LLM may choose to recursively perform the RAG process, applying query improvement techniques as needed to improve the relevancy of results. Finally, adaptive retrieval utilizes fine-tuned LLMs to intelligently decide if the RAG process is necessary. 


**NOT FINISHED - Going into agentic application next and then finishing with my implementation + research question**



In this study, knowledge base documents were chunked based on document structure, then on chunk size, enabling a 20% context overlap. This study also implemented metadata enrichment. Source hierarchy and header information were injected into chunk content to improve dense embeddings and increase retrieval precision.

 This study expands upon both @gao2024rag and @BlendedRAG, implementing a hybrid index, which utilizes dense embeddings and sparse-encoded embeddings.


### Working Literature Review (To Be Deleted)

(This is way too long and will be shortened once I get a better idea of the flow).

@lewis2020retrieval revolutionized neural language models by providing an architecture through which a sequence-2-sequence (seq2seq) model can access knowledge that it had not previously been trained upon, expanding the model’s memory and reducing hallucinations. They combined a pre-trained retrieval model, which included a query encoder and a document index, with a pre-trained seq2seq model. This architecture is termed “retrieval-augmented generation.” The process is as follows: The retrieval model encodes the user’s query and maps the query to the appropriate results using Maximum Inner Product Search (MIPS). The retriever returns the top-K results from the document index to the generation model. The generation model then utilizes probability to determine its response, using the original user’s query and the information returned by the retrieval model. Lewis et al., tested two different techniques for determining the responses returned by the generation models: RAG-Sequence and RAG-Token. In RAG-Sequence, the model treats each of the k-returned documents as separate entities, forming a response based upon each independent document. Then, the model evaluates the probability of each response against all of the documents, and returns the response with the highest probability. The RAG-Token technique is much more flexible, allowing the generation model to use all documents in its response, as its response is crafted per token. The model looks at each document individually and returns a list of tokens and their corresponding probabilities based upon the individual document, the user’s query, and all previous tokens. The model sums up these probabilities for all k-documents, then selects the token with the highest cumulative probability. It repeats this process until the response is complete. An important feature of their architecture is that the query encoder (BERTq) and the generation model (BART) were fine-tuned together, using stochastic gradient descent with Adam to minimize the cost function. The architecture and generation techniques were tested on four different domain subsets: open-domain question answering, abstractive question answering, jeopardy question generation, and fact verification. The RAG architecture beat state of the art models for open-domain QA, with the RAG-Sequence technique performing better overall than the RAG-Token model. While the RAG architecture did not beat state of the art models in abstractive question answering, it did exceed BART’s performance (which is equivalent to solely the generation part of the RAG architecture), illustrating the advantages of combining retrieval with generation. Again, RAG-Sequence outperformed RAG-Token. When completing the Jeopardy question generation tasks, the RAG model outperformed the BART model; in this scenario, the RAG-Token model outperformed the RAG-Sequence model. The authors propose that this improvement in performance is because the answers to Jeopardy questions may be split over several documents. Finally, human assessments for the Jeopardy Question Generation task find that RAG responses are both more factually accurate and more specific than BART responses. Overall, the paper’s authors propose a new architecture, which combines a pre-trained retriever model with a pre-trained generation model, which demonstrates performance near or exceeding state of the art models in four different domains. The authors demonstrate that in open-domain QA or abstractive QA, RAG-Sequence models outperform RAG-Token models, while RAG-Token models outperform in scenarios where an answer is spread across multiple sources. They also demonstrate that the information retrieved can be supplemented by the information generated by the generation model’s parametric knowledge, increasing overall performance. Finally, the authors show that humans prefer the responses generated by a RAG architecture over responses generated by a pure generation model. This paper is revolutionary in that it overcomes the knowledge-cutoff date problem present in many early neural language models. By updating the information contained within the dense index, the model will subsequently update its responses, without needing to retrain the generation model. This is a much cheaper option. Additionally, it provides some transparency in terms of where information comes from. Whereas factual knowledge is stored somewhere within the layers of a neural network, information obtained using a retriever has a citation, improving the verifiability of responses. 


@gao2024rag identifies three technological paradigms related to RAG: Naive RAG, Advanced RAG, and Modular RAG. Through the systematic review of over 100 RAG-related studies, they comprehensively document the technologies used in each part of the RAG process: Retrieval, Augmentation, and Generation, as well as the optimization technologies implemented in Advanced RAG and Modular RAG. The authors finish by documenting common assessment frameworks and benchmarks used to evaluate RAG systems. 

The Naive RAG process is identical to the one championed by @lewis2020retrieval. The documents are chunked, vectorized, and uploaded to a vector database. The user query is vectorized using the same embedding model. The retriever retrieves the top-k documents by determining which documents are most mathematically similar to the user’s query. While Naive RAG’s performance is high compared to non-RAG enabled LLMs, naive RAG systems can suffer by retrieving irrelevant documents, utilizing parametric memory to hallucinate answers, or have difficulty synthesizing information in an efficient manner. Advanced RAG implements both pre-retrieval and post-retrieval optimization techniques to overcome the shortcomings of Naive RAG. Pre-retrieval includes both improving the processing of indexing the document knowledge base and improving upon the user query itself. Post-retrieval improvements include document re-ranking and reducing the context to prevent from overloading the LLM. 

Both Naive RAG and Advanced RAG flow linearly: The system receives the user’s query, retrieves the relevant documents, returns the relevant context to the LLM, which generates a result. Modular RAG instead provides the capability to alter this flow, building upon the foundational principles established by both Naive RAG and Advanced RAG. It utilizes techniques such as routing, multi-query parallel searches, hybrid retrieval techniques, or hypothetical document embeddings (HyDE). Routing allows the system to determine intent and direct queries appropriately, while expanding the original query into multiple queries increases the retriever’s context. Hypothetical Document Embeddings, or HyDE, compares similarity between the indexed documents and an LLM-generated answer, aiming to improve precision. There are many other techniques that can be implemented through Modular RAG systems, which aims to improve context, flexibility, and overall efficiency.

For each stage of a RAG pipeline, the authors meticulously document the design decisions which can majorly impact system performance and discuss appropriate optimization strategies. Retrieval performance can be impacted by the data format (structured, semi-structured, or unstructured) as well as retrieval unit granularity - the size of content chunks. Retrieval optimization opportunities generally fall into one of four categories: index optimization, query optimization, embedding optimization, and the use of an adapter.

Indexing optimization focuses on how data is stored. Strategies include recursively splitting a document using structure or using sliding windows to overlap context between chunks, preventing context truncation. The inclusion of metadata, such as file structure or dates, can provide additional filtering context. One technique involves using an LLM to generate a summary of the paragraph, which can be appended  as metadata. Finally, the indexes can be structured in hierarchical formats or knowledge graphs, better capturing relationships between data. 

Query optimization strategies are employed to refine the user’s intent before retrieval. Query expansion utilizes an LLM to break the query into multiple queries, providing additional context or providing separation between contextually different aspects of the user’s query. Query transformation rewrites the user’s query entirely, either by completing a full rewrite, utilizing the HyDE technique discussed above, or allowing the LLM to generate a high-level concept question in response to the user’s query - both of which are used when retrieving results. Finally, query routing directs the user’s query to different RAG pipelines. For example, a router may direct a domain-specific query to a RAG pipeline which utilizes lexical search for exact matching, while it may route a more general query to a pipeline enabled with semantic search capabilities. 

The embedding model is responsible for vectorizing the documents and the user’s query. Vectors may be sparse, used for exact keyword matching, or dense, which allows the vector to capture contextual meaning. The authors suggest two embedding optimization techniques: Using a hybrid model, using both sparse and dense representations, or fine-tuning an embedding model for knowledge-intensive, domain-specific scenarios. However, in cases where the embedding model cannot be fine-tuned, the authors suggest the use of a light-weight adapter, which is a small neural network trained to align the output of the embedding model with domain-specific context. 

In the Generation process, the LLM synthesizes the retrieved results into an answer which is returned to the user. The authors highlight that it is generally not a good idea to return the results as-is and recommend curating the context before returning the information to the user. This may involve re-ranking outputs to ensure that the most relevant answer is provided first. Other post-retrieval techniques include context compression, using a small language model (SLM) to pre-filter noise, or using an LLM to evaluate the content prior to returning an answer to the user. Finally, the generator LLM itself can be fine-tuned to better understand domain-specific information or to receive retriever outputs in varying formats. 

Basic RAG pipelines are linear in flow, while Modular RAG improves performance by allowing flexibility in the data flow. Augmentation techniques, in the form of advanced architecture, capitalize on this by enabling loops. Iterative retrieval initializes with the usual linear process. The generated response is then appended to the initial user query and re-sent through the pipeline, enhancing the context. This process repeats until the number of iterations exceeds a pre-defined threshold. 

Recursive retrieval again initializes with a complete cycle; however, it then allows the generator LLM to judge the relevance of the most recent results. The LLM may choose to recursively perform the RAG process, performing query transformation or decomposition as needed to improve the relevancy of results. Finally, adaptive retrieval utilizes fine-tuned LLMs to intelligently decide if the RAG process is necessary.

The authors finish by systematically examining the metrics upon which RAG systems are evaluated. They mention that while task-specific metrics are often used (such as BLEU or ROUGE), RAG systems inherently have two objectives: retrieval and generation. These targets should be evaluated separately. The retrieved results should be evaluated for precision and specificity, while the generated text should be evaluated against the context of what was returned (faithfulness) and its relevancy to the initial user query. This trio can be referred to as a model’s quality scores. The authors also highlight four abilities that a model should have: It should be able to ignore documents which contain contextually-relevant but non-substantive information, it should refrain from generating an answer when the returned results are not useful, it should be able to synthesize information efficiently across multiple documents, and it should be able to identify known inaccuracies. 

@BlendedRAG explore how to improve the retrieval effectiveness of RAG systems by "blending" different indexing techniques, including: standard sparse indexing, dense indexing, and sparse encoder-based vector models. Standard sparse indexing, colloquially known as keyword-matching or lexical search, assigns relevance based upon how many times a keyword appears in a document relative to the collection of documents. In this paper, BM25 was the selected sparse index. Dense indexes work by comparing the vectorized form of the user's query to a collection of vectorized documents, selecting the top k documents using cosine-similarity. This method is efficient for capturing context. The authors used the k-Nearest Neighbor (KNN) algorithm for the dense retrieval. Sparse-encoder indexes expand upon typical sparse indexes by utilizing a model to expand both the user's query and the collection of documents with related words before searching, increasing the index size and improving the chance of identifying relevant information. The authors utilized the Elastic Learned Sparse Encoder (ELSER) in this experiment.

The authors also test six distinct strategies to search the knowledge corpus for information related to the user's query: Match Query, Bool Prefix, Cross Fields, Most Fields, Best Fields, and Phrase Prefix. Match Query is a standard lexical search, as explained above. Bool Prefix treats each word as an individual term, except for the last term, which it treats as a prefix. It then performs a Boolean search, returning a document if it matches any of the terms or the prefix. Cross Fields treats multiple fields (e.g., title, body) as a single text block and selects the document(s) with the highest cumulative score, whereas Most Fields prioritizes documents where the keyword appears across the highest number of fields (e.g., in both the title and the body paragraph). Best Fields calculates a score for each field and selects only the document's field which has the highest score. Phrase Prefix is similar to Best Fields; however, it prioritizes grouping the keywords together as a phrase. The authors tested each of these six search query methods across the three index types, selecting the combinations of index and query methods which had the highest accuracy on the NQ, Trec-Covid, and HotPotQA datasets. 

The authors ultimately found the following six combinations to have superior performance: Sparse Encoder + Match Query, BM25 + Match Query, KNN + Match Query, BM25 + Best Field, KNN + Best Field, and Sparse Encoder + Best Field. Benchmarking these against state-of-the-art systems using the NDCG@10 metric, the authors demonstrated that semantic-based hybrid methods consistently outperform both the current SoTA and keyword-based hybrid methods on the NQ, Trec-Covid and SqUAD datasets.The Sparse Encoder-based semantic search, when combined with the 'Best Fields' query, demonstrated superior performance, even when compared to RAG pipelines specifically fine-tuned for those datasets.

Traditionally, RAG systems were evaluated end-to-end, meaning that the system's accuracy was determined by the quality of content generated by the LLM. @salemi2024eRAG begin by highlighting the disadvantages of traditional end-to-end RAG evaluation methods, including: a lack of transparency, increased computational complexity, and the inability to generate document-level feedback which could be used to improve the retriever's performance. Historically, human annotations have been used to evaluate the documents returned for relevancy; however, the authors challenge this paradigm, noting that human annotations can be costly to obtain. Furthermore, applying human relevance to a document assumes alignment with what the LLM deems to be relevant. The authors substantiate their challenge with the observation that retrieval systems with human annotations have a "minor correlation" to the success of the overall pipeline. 

To address this, @salemi2024eRAG propose eRAG, a retriever evaluation method. eRAG is a system where the LLM individually processes each document returned by the retriever and generates an answer. Metrics, such as Exact Match or ROUGE, can be employed to generate a utility score, based upon the LLM's ability to answer the user's query using only that document's content. In the case where Exact Match is employed, the LLM is a binary classifier, returning 1 if the document is relevant and 0 if not. The accuracy scores can then be aggregated set-wise, and the retrieved result set can be evaluated using standard metrics, including precision, recall, mean average precision, mean reciprocal rank, normalized discounted cumulative gain, and hit rate. 

Mathematically, @salemi2024eRAG demonstrate that this method of evaluation is more computationally efficient, reducing runtime complexity from $O(lk^2d^2)$ to $O(lkd^2)$, where k is the number of documents, d is the average length of each document, and l is the length of the generated output. Standard transformers apply a runtime "penalty" to long inputs, scaling the cost quadratically. Running the LLM k times, rather than combining all k documents into a single input, helps to avoid this quadratic cost penalty.

To validate their approach, the authors ran experiments on five different datasets (Natural Questions (NQ), TriviaQA, HotpotQA, FEVER, and Wizard of Wikipedia (WoW)) from the KILT benchmark, utilizing the recommended metrics for each. The authors used the open-source LLM, Mistral, to generate document-level relevancy scores and then measured the correlation with the downstream RAG pipeline performance, which utilizes the same retriever and the T5-small with Fusion-in-Decoder as the generator. 

The authors found that eRAG consistently outperformed more traditional systems, including the KILT Provenance and Relevance Annotation with LLM. The eRAG method also outperformed all other methods as the number of documents retrieved increased. However, all evaluation metrics indicated decreasing correlation as the number of documents increased, which was attributed to the fact that each architecture assesses relevancy independently, while the RAG pipeline's generator uses information from all sources. The authors also tested the eRAG performance against RAG pipelines with increasing LLM size, finding no evidence that performance differs depending upon downstream LLM capabilities. Finally, the authors compared eRAG evaluation time and memory consumption to that of traditional end-to-end evaluation methods, finding that, on average, eRAG is 2.5 times faster and significantly more memory-efficient, depending on whether the evaluation process is being performed at a document-level or query-level. Overall, @salemi2024eRAG demonstrate an extremely efficient methodology to evaluate retriever performance compared to traditional end-to-end methods.

@AgenticAI2025Jagga evaluates the evolution of agentic artificial intelligence (AI) systems, focusing on the architectural frameworks and core components which enable autonomous perception and action-oriented response. Agentic AI has advanced significantly in comparison to its predecessors; agentic AI is no longer reactive, instead it employs the capability to sense changes in its environment and respond autonomously. While the autonomy given to agents differs depending upon the domain, the systems are capable of full autonomy, reflecting a marked advancement in the AI realm. 

The architectural design of an agentic AI system is dependent upon the needs of its application. The author emphasizes that operational effectiveness is dependent upon alignment between the agent's planning mechanisms and the stability of its deployment environment. More stable environments favor deliberative architectures that enable long-term planning, while volatile environments require reactive systems. As such, the three most common design paradigms are deliberative systems, reactive response systems, and hybrid systems which share both reactive and proactive characteristics.

The author evaluates seven core architectural components, beginning with perception modules. These serve as the input for sensory information. The author argues that complex modules, or those that employ multi-layer input transformations to derive contextual meaning from simple inputs, outperform context-free systems in ambiguous scenarios. Following perception modules, knowledge representation systems are critical to agentic performance and must be accurate, fast, and easy to maintain. The author highlights that hierarchical-based storage systems perform well in semantic domains, while systems which are able to access previous interactions (case-based reasoning) excel in experience-driven applications. Planning and reasoning engines follow, with the author highlighting multiple techniques including hierarchical decomposition, partial-order planning, and anytime algorithms. Hierarchical decomposition allows the agent to break a single task into several subsequent tasks. An agent which utilizes partial-order planning recognizes that the sequence of task execution can be flexible, while an agent which utilizes an anytime algorithm to plan implements a plan and refines as needed. However, the author notes that adaptive agents, or agents which can adjust their planning mechanism in response to the task, are most successful.

The next core component is the agent's decision-making framework, which allows the agent to balance competing priorities and handle uncertainty. Techniques range from rule-based systems to probabilistic models that determine the optimal course of action. As the name suggests, action execution mechanisms are the underlying code which enable the agent to carry out its necessary tasks (e.g., the API call). These mechanisms are not isolated; rather, they must operate as closed-loops which monitor performance and relay feedback as needed to higher-level systems. Systems which have robust error-handling and performance-monitoring loops demonstrate higher performance. Learning and adaptation systems allow an agent to improve its performance based upon feedback, with research suggesting that adaptive systems have improved operational performance when compared to non-adaptive systems. Communications protocols determine how agents in multi-agent systems communicate with one another and distribute tasks, as well as how the agents communicate with the human. The author emphasizes that a well-designed protocol is key to operational efficiency.

The author also explores multi-agent systems (MAS), likening them to a team of experts where individual domain contributions are necessary to solve a complex problem. Research indicates that a well-designed MAS can outperform a monolithic system, outfitted with similar computational resources, emphasizing the importance of multi-agent systems. Challenges to an MAS include task decomposition, resource allocation, and conflict resolution, with several common architectures being employed to mitigate these challenges. Hierarchical coordination mimics organizational hierarchy, with the higher agent instructing a lower-level agent and reviewing its performance. Market-based coordination is founded on economic principles, allowing agents to negotiate for resources and tasks without centralized control. Consensus-based coordination excels in scenarios with high uncertainty, allowing agents to reach a decision based on a collective-decision making. Finally, emergent coordination, often referred to as "swarming", also lacks centralized control, instead allowing the system to self-organize. This model is inspired by biological systems (e.g., an ant colony), with each agent having limited visibility and performing only simple tasks. This model provides failure-resilience, as other agents can pick up tasks if one agent is to fail, enabling exceptional scalability. The author emphasizes the importance of tailoring the coordination model to application needs.

The agentic workflow provides significant advantages over traditional automation, especially in highly variable scenarios. Effective agentic workflows employ efficient task decomposition, using information requirements as a natural boundary. Dynamic resource allocation, or using only what is needed, is key to optimizing efficiency and reducing overall costs. The author again highlights the need for robust exception handling, designing the system with fall-back mechanisms and graceful failures, as well as progress monitoring, which allows the system to adjust behavior depending on interim results. Finally, a well-designed agentic workflow provides visibility into the agentic thought process and provides the human with the ability to provide feedback or override decisions.

Technical challenges include scalability, explainability, and robustness. As task complexity increases and the number of agents increase, computational requirements also increase, which proves an issue when trying to maintain cost efficiency at scale. Additionally, agentic workflows which utilize neural networks can lack transparency in reasoning, which proves an issue when clear justification is required (e.g., legal domain). Agentic workflows struggle with reliability, specifically in situations where operation deviates from normal conditions. This is a significant challenge, particularly in industrial scenarios. Finally, agentic systems struggle with long-term planning, as evaluating an infinite number of possibilities is computationally expensive. As such, current implementations often employ approximation techniques, which increase computational efficiency but reduce the ability to forecast long-term effects to current actions. As autononomous systems continue to gain popularity, the author highlights the need to balance performance with safety and ethical design. Specifically, governance frameworks are needed to ensure that the agentic behavior remains aligned with human intention. Human overrides are a safety-critical mechanism, ensuring safety when agentic behavior deviates from intent. Additionally, the transparency required for technical evaluation must be balanced with not revealing sensitive or legally protected information, providing a challenge for highly-secure domains (e.g., military, health-care).

While not part of the literature review, this article gave me several ideas on improvements to my agentic design. Beginning with the perception module, as previously discussed, query pre-processing in a RAG system can markedly improve performance. This article's emphasis on context enhancement highlights the need to apply some type of pre-processing, including HyDE or query expansion. With regards to the knowledge representation system, I question whether the agent would benefit from the ability to access historical queries and answers, as an academic chatbot is experience-driven. Currently, the knowledge base incorporates hierarchical design, as the header hierarchy and source metadata has been added to the content prior to vectorization. With regards to the planning and reasoning engines, I believe you can implement the adaptive approach, which may be particularly useful for the coordination agent. Allowing the coordination agent to identify the complexity of the task, decomposing and directing as needed rather than decomposing all tasks is more computationally efficient and I believe would generate better results. Utilizing a post-processing agent for the RAG system, which evaluates the applicability of each response, is a potential way to improve the AI agent's decision-making (LLM as a judge, but in a manner similar to @BlendedRAG). Alternatively, if we choose to employ query expansion or HyDE for query pre-processing, would it make sense to add one additional agent, which evaluates the quality of the expanded query or generated answer, prior to performing the vector similarity search against the vector DB. To improve the action-execution mechanism, all tools should have robust error handling, with retry mechanisms and appropriate error messages, which allow the user and/or coordination agent to respond as necessary, rather than allowing a tool to fail silently. While I still believe the hierarchical coordination model to be the optimal implementation for a QA-system, the consensus-based coordination model may prove to be useful, especially when utilizing LLM-as-a-judge methods. Rather than alllowing one LLM to determine the relevance of returned results, it would be interesting to test utilizing two or more LLMs, averaging the relevancy scores, and returning results based upon a consensus. 

@AgenticRAGwHumans proposes a novel approach for agentic retrieval-augmented generation (RAG) systems by integrating domain-specific experts directly into the retrieval process with the goal of improving advanced domain-specific RAG pipelines.

Traditional RAG architectures utilize a linear retriever-generator pipeline. Subsequent advancements introduced dynamicity, such as iterative loops and re-ranking capabilities, to increase the accuracy and relevancy of returned results. More recently, agentic RAG has emerged, leveraging the autonomous planning and decision-making capabilities of AI agents. An orchestration agent decides when to enact the RAG process and break down complex tasks into simpler tasks using a pre-defined reasoning framework. The retriever module queries the appropriate knowledge base. The module may be implemented as a single, or set of tools, or it may be implemented as an agent who employs reasoning to determine which knowledge base to query and how best to optimize the query (e.g., generate an SQL query for database KB). The results are returned based upon similarity, or some other search metric depending upon the retrieval source. The generator module, also implemented as an agent, processes the information prior to returning it to the user-facing agent. The generator module may employ algorithms such as re-ranking or other optimization algorithms designed to improve the response and/or limit the context passed to the user-facing model. A guardrail module is an agent employed to ensure that the information received by the user is safe and ethically-sourced. For example, the module helps ensure the user is only able to access information for which he or she is permitted access.

However, technically-advanced users may pose specific questions which lack context, or possess an "implicit understanding that is difficult to articulate or extract," with both scenarios proving an issue even in the best-designed RAG system. As such, @AgenticRAGwHumans proposes the inclusion of domain-experts into the retrieval process, who possess the inherent context or tacit knowledge required to answer advanced questions. The proposed pipeline extends the current paradigm, by allowing the agent to request feedback from the user throughout its pipeline, specifically in the planning and action stages, allowing the agent to capitalize on the user's inherent knowledge, which may not be initially provided. Furthermore, the author recommends extending the pipeline to allow the agentic system to query and request decision-reviews from available human domain-experts, proving useful in high-stakes applications (e.g., law). The information provided by the human expert should be retained for use in future queries (e.g., case-based reasoning). However, it must be decided whether this tacit knowledge should be included into the main knowledge base or stored separately. Additionally, if the information is stored separately, the human feedback can be stored in multiple formats: vector dB, knowledge graph, etc., Deciding the optimal format is dependent upon the "nature and volume" of the feedback.

The authors note that human-inclusion requires architectural modifications to both the online and offline RAG processes. The primary implementation challenge lies in determining when human feedback is necessary, since feedback could theoretically be provided at any point. The authors argue that human feedback should be reserved for situations with high uncertainty, closely ranked retrieval results, or cases lacking historical precedence.
____

This research aims to implement techniques from both Advanced RAG and Modular RAG technological paradigms in an agentic context, utilizing different agentic architectures to identify which combination of techniques provides the optimal performance in an academic environment. 

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.


## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{python}
import pandas as pd
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References

