---
title: "Optimizing Agentic Retrieval-Augmented Generation: An Analysis of Single vs. Multi-Agent Workflows"
author: "Cheyanne Allred-Lopez (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

## Introduction

For Symbolic Regression [@wang2019symbolic] *This is my work and I want
to add more work...*

Cite new paper [@su2012linear]

### Working Literature Review (To Be Deleted)

(This is way too long and will be shortened once I get a better idea of the flow).

@lewis2020retrieval revolutionized neural language models by providing an architecture through which a sequence-2-sequence (seq2seq) model can access knowledge that it had not previously been trained upon, expanding the model’s memory and reducing hallucinations. They combined a pre-trained retrieval model, which included a query encoder and a document index, with a pre-trained seq2seq model. This architecture is termed “retrieval-augmented generation.” The process is as follows: The retrieval model encodes the user’s query and maps the query to the appropriate results using Maximum Inner Product Search (MIPS). The retriever returns the top-K results from the document index to the generation model. The generation model then utilizes probability to determine its response, using the original user’s query and the information returned by the retrieval model. Lewis et al., tested two different techniques for determining the responses returned by the generation models: RAG-Sequence and RAG-Token. In RAG-Sequence, the model treats each of the k-returned documents as separate entities, forming a response based upon each independent document. Then, the model evaluates the probability of each response against all of the documents, and returns the response with the highest probability. The RAG-Token technique is much more flexible, allowing the generation model to use all documents in its response, as its response is crafted per token. The model looks at each document individually and returns a list of tokens and their corresponding probabilities based upon the individual document, the user’s query, and all previous tokens. The model sums up these probabilities for all k-documents, then selects the token with the highest cumulative probability. It repeats this process until the response is complete. An important feature of their architecture is that the query encoder (BERTq) and the generation model (BART) were fine-tuned together, using stochastic gradient descent with Adam to minimize the cost function. The architecture and generation techniques were tested on four different domain subsets: open-domain question answering, abstractive question answering, jeopardy question generation, and fact verification. The RAG architecture beat state of the art models for open-domain QA, with the RAG-Sequence technique performing better overall than the RAG-Token model. While the RAG architecture did not beat state of the art models in abstractive question answering, it did exceed BART’s performance (which is equivalent to solely the generation part of the RAG architecture), illustrating the advantages of combining retrieval with generation. Again, RAG-Sequence outperformed RAG-Token. When completing the Jeopardy question generation tasks, the RAG model outperformed the BART model; in this scenario, the RAG-Token model outperformed the RAG-Sequence model. The authors propose that this improvement in performance is because the answers to Jeopardy questions may be split over several documents. Finally, human assessments for the Jeopardy Question Generation task find that RAG responses are both more factually accurate and more specific than BART responses. Overall, the paper’s authors propose a new architecture, which combines a pre-trained retriever model with a pre-trained generation model, which demonstrates performance near or exceeding state of the art models in four different domains. The authors demonstrate that in open-domain QA or abstractive QA, RAG-Sequence models outperform RAG-Token models, while RAG-Token models outperform in scenarios where an answer is spread across multiple sources. They also demonstrate that the information retrieved can be supplemented by the information generated by the generation model’s parametric knowledge, increasing overall performance. Finally, the authors show that humans prefer the responses generated by a RAG architecture over responses generated by a pure generation model. This paper is revolutionary in that it overcomes the knowledge-cutoff date problem present in many early neural language models. By updating the information contained within the dense index, the model will subsequently update its responses, without needing to retrain the generation model. This is a much cheaper option. Additionally, it provides some transparency in terms of where information comes from. Whereas factual knowledge is stored somewhere within the layers of a neural network, information obtained using a retriever has a citation, improving the verifiability of responses. 


@gao2024rag identifies three technological paradigms related to RAG: Naive RAG, Advanced RAG, and Modular RAG. Through the systematic review of over 100 RAG-related studies, they comprehensively document the technologies used in each part of the RAG process: Retrieval, Augmentation, and Generation, as well as the optimization technologies implemented in Advanced RAG and Modular RAG. The authors finish by documenting common assessment frameworks and benchmarks used to evaluate RAG systems. 

The Naive RAG process is identical to the one championed by @lewis2020retrieval. The documents are chunked, vectorized, and uploaded to a vector database. The user query is vectorized using the same embedding model. The retriever retrieves the top-k documents by determining which documents are most mathematically similar to the user’s query. While Naive RAG’s performance is high compared to non-RAG enabled LLMs, naive RAG systems can suffer by retrieving irrelevant documents, utilizing parametric memory to hallucinate answers, or have difficulty synthesizing information in an efficient manner. Advanced RAG implements both pre-retrieval and post-retrieval optimization techniques to overcome the shortcomings of Naive RAG. Pre-retrieval includes both improving the processing of indexing the document knowledge base and improving upon the user query itself. Post-retrieval improvements include document re-ranking and reducing the context to prevent from overloading the LLM. 

Both Naive RAG and Advanced RAG flow linearly: The system receives the user’s query, retrieves the relevant documents, returns the relevant context to the LLM, which generates a result. Modular RAG instead provides the capability to alter this flow, building upon the foundational principles established by both Naive RAG and Advanced RAG. It utilizes techniques such as routing, multi-query parallel searches, hybrid retrieval techniques, or hypothetical document embeddings (HyDE). Routing allows the system to determine intent and direct queries appropriately, while expanding the original query into multiple queries increases the retriever’s context. Hypothetical Document Embeddings, or HyDE, compares similarity between the indexed documents and an LLM-generated answer, aiming to improve precision. There are many other techniques that can be implemented through Modular RAG systems, which aims to improve context, flexibility, and overall efficiency.

For each stage of a RAG pipeline, the authors meticulously document the design decisions which can majorly impact system performance and discuss appropriate optimization strategies. Retrieval performance can be impacted by the data format (structured, semi-structured, or unstructured) as well as retrieval unit granularity - the size of content chunks. Retrieval optimization opportunities generally fall into one of four categories: index optimization, query optimization, embedding optimization, and the use of an adapter.

Indexing optimization focuses on how data is stored. Strategies include recursively splitting a document using structure or using sliding windows to overlap context between chunks, preventing context truncation. The inclusion of metadata, such as file structure or dates, can provide additional filtering context. One technique involves using an LLM to generate a summary of the paragraph, which can be appended  as metadata. Finally, the indexes can be structured in hierarchical formats or knowledge graphs, better capturing relationships between data. 

Query optimization strategies are employed to refine the user’s intent before retrieval. Query expansion utilizes an LLM to break the query into multiple queries, providing additional context or providing separation between contextually different aspects of the user’s query. Query transformation rewrites the user’s query entirely, either by completing a full re-write, utilizing the HyDE technique discussed above, or allowing the LLM to generate a high-level concept question in response to the user’s query - both of which are used when retrieving results. Finally, query routing directs the user’s query to different RAG pipelines. For example, a router may direct a domain-specific query to a RAG pipeline which utilizes lexical search for exact matching, while it may route a more general query to a pipeline enabled with semantic search capabilities. 

The embedding model is responsible for vectorizing the documents and the user’s query. Vectors may be sparse, used for exact keyword matching, or dense, which allows the vector to capture contextual meaning. The authors suggest two embedding optimization techniques: Using a hybrid model, using both sparse and dense representations, or fine-tuning an embedding model for knowledge-intensive, domain-specific scenarios. However, in cases where the embedding model cannot be fine-tuned, the authors suggest the use of a light-weight adapter, which is a small neural network trained to align the output of the embedding model with domain-specific context. 

In the Generation process, the LLM synthesizes the retrieved results into an answer which is returned to the user. The authors highlight that it is generally not a good idea to return the results as-is and recommend curating the context before returning the information to the user. This may involve re-ranking outputs to ensure that the most relevant answer is provided first. Other post-retrieval techniques include context compression, using a small language model (SLM) to pre-filter noise, or using an LLM to evaluate the content prior to returning an answer to the user. Finally, the generator LLM itself can be fine-tuned to better understand domain-specific information or to receive retriever outputs in varying formats. 

Basic RAG pipelines are linear in flow, while Modular RAG improves performance by allowing flexibility in the data flow. Augmentation techniques, in the form of advanced architecture, capitalize on this by enabling loops. Iterative retrieval initializes with the usual linear process. The generated response is then appended to the initial user query and re-sent through the pipeline, enhancing the context. This process repeats until the number of iterations exceeds a pre-defined threshold. 

Recursive retrieval again initializes with a complete cycle; however, it then allows the generator LLM to judge the relevance of the most recent results. The LLM may choose to recursively perform the RAG process, performing query transformation or decomposition as needed to improve the relevancy of results. Finally, adaptive retrieval utilizes fine-tuned LLMs to intelligently decide if the RAG process is necessary.

The authors finish by systematically examining the metrics upon which RAG systems are evaluated. They mention that while task-specific metrics are often used (such as BLEU or ROUGE), RAG systems inherently have two objectives: retrieval and generation. These targets should be evaluated separately. The retrieved results should be evaluated for precision and specificity, while the generated text should be evaluated against the context of what was returned (faithfulness) and its relevancy to the initial user query. This trio can be referred to as a model’s quality scores. The authors also highlight four abilities that a model should have: It should be able to ignore documents which contain contextually-relevant but non-substantive information, it should refrain from generating an answer when the returned results are not useful, it should be able to synthesize information efficiently across multiple documents, and it should be able to identify known inaccuracies. 

@BlendedRAG explore how to improve the retrieval effectiveness of RAG systems by "blending" different indexing techniques, including: standard sparse indexing, dense indexing, and sparse encoder-based vector models. Standard sparse indexing, colloquially known as keyword-matching or lexical search, assigns relevance based upon how many times a keyword appears in a document relative to the collection of documents. In this paper, BM25 was the selected sparse index. Dense indexes work by comparing the vectorized form of the user's query to a collection of vectorized documents, selecting the top k documents using cosine-similarity. This method is efficient for capturing context. The authors used the k-Nearest Neighbor (KNN) algorithm for the dense retrieval. Sparse-encoder indexes expand upon typical sparse indexes by utilizing a model to expand both the user's query and the collection of documents with related words before searching, increasing the index size and improving the chance of identifying relevant information. The authors utilized the Elastic Learned Sparse Encoder (ELSER) in this experiment.

The authors also test six distinct strategies to search the knowledge corpus for information related to the user's query: Match Query, Bool Prefix, Cross Fields, Most Fields, Best Fields, and Phrase Prefix. Match Query is a standard lexical search, as explained above. Bool Prefix treats each word as an individual term, except for the last term, which it treats as a prefix. It then performs a Boolean search, returning a document if it matches any of the terms or the prefix. Cross Fields treats multiple fields (e.g., title, body) as a single text block and selects the document(s) with the highest cumulative score, whereas Most Fields prioritizes documents where the keyword appears across the highest number of fields (e.g., in both the title and the body paragraph). Best Fields calculates a score for each field and selects only the document's field which has the highest score. Phrase Prefix is similar to Best Fields; however, it prioritizes grouping the keywords together as a phrase. The authors tested each of these six search query methods across the three index types, selecting the combinations of index and query methods which had the highest accuracy on the NQ, Trec-Covid, and HotPotQA datasets. 

The authors ultimately found the following six combinations to have superior performance: Sparse Encoder + Match Query, BM25 + Match Query, KNN + Match Query, BM25 + Best Field, KNN + Best Field, and Sparse Encoder + Best Field. Benchmarking these against state-of-the-art systems using the NDCG@10 metric, the authors demonstrated that semantic-based hybrid methods consistently outperform both the current SoTA and keyword-based hybrid methods on the NQ, Trec-Covid and SqUAD datasets.The Sparse Encoder-based semantic search, when combined with the 'Best Fields' query, demonstrated superior performance, even when compared to RAG pipelines specifically fine-tuned for those datasets.


____

This research aims to implement techniques from both Advanced RAG and Modular RAG technological paradigms in an agentic context, utilizing different agentic architectures to identify which combination of techniques provides the optimal performance in an academic environment. 

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{python}
import pandas as pd
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References

