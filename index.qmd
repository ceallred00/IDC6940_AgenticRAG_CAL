---
title: "Optimizing Agentic Retrieval-Augmented Generation: An Analysis of Single vs. Multi-Agent Workflows"
author: "Cheyanne Allred-Lopez (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

## Introduction

The introduction should:

-   Develop a storyline that captures attention and maintains interest.

-   Your audience is your peers

-   Clearly state the problem or question you're addressing.

<!-- -->

-   Introduce why it is relevant needs.

-   Provide an overview of your approach.

Example of writing including citing references:

*This is an introduction to ..... regression, which is a non-parametric
estimator that estimates the conditional expectation of two variables
which is random. The goal of a kernel regression is to discover the
non-linear relationship between two random variables. To discover the
non-linear relationship, kernel estimator or kernel smoothing is the
main method to estimate the curve for non-parametric statistics. In
kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].
The PCA [@daffertshofer2004pca]*. Topology can be used in machine
learning [@adams2021topology]

For Symbolic Regression [@wang2019symbolic] *This is my work and I want
to add more work...*

Cite new paper [@su2012linear]

### Working Literature Review (To Be Deleted)

(This is way too long and will be shortened once I get a better idea of the flow).

@lewis2020retrieval revolutionized neural language models by providing an architecture through which a sequence-2-sequence (seq2seq) model can access knowledge that it had not previously been trained upon, expanding the model’s memory and reducing hallucinations. They combined a pre-trained retrieval model, which included a query encoder and a document index, with a pre-trained seq2seq model. This architecture is termed “retrieval-augmented generation.” The process is as follows: The retrieval model encodes the user’s query and maps the query to the appropriate results using Maximum Inner Product Search (MIPS). The retriever returns the top-K results from the document index to the generation model. The generation model then utilizes probability to determine its response, using the original user’s query and the information returned by the retrieval model. Lewis et al., tested two different techniques for determining the responses returned by the generation models: RAG-Sequence and RAG-Token. In RAG-Sequence, the model treats each of the k-returned documents as separate entities, forming a response based upon each independent document. Then, the model evaluates the probability of each response against all of the documents, and returns the response with the highest probability. The RAG-Token technique is much more flexible, allowing the generation model to use all documents in its response, as its response is crafted per token. The model looks at each document individually and returns a list of tokens and their corresponding probabilities based upon the individual document, the user’s query, and all previous tokens. The model sums up these probabilities for all k-documents, then selects the token with the highest cumulative probability. It repeats this process until the response is complete. An important feature of their architecture is that the query encoder (BERTq) and the generation model (BART) were fine-tuned together, using stochastic gradient descent with Adam to minimize the cost function. The architecture and generation techniques were tested on four different domain subsets: open-domain question answering, abstractive question answering, jeopardy question generation, and fact verification. The RAG architecture beat state of the art models for open-domain QA, with the RAG-Sequence technique performing better overall than the RAG-Token model. While the RAG architecture did not beat state of the art models in abstractive question answering, it did exceed BART’s performance (which is equivalent to solely the generation part of the RAG architecture), illustrating the advantages of combining retrieval with generation. Again, RAG-Sequence outperformed RAG-Token. When completing the Jeopardy question generation tasks, the RAG model outperformed the BART model; in this scenario, the RAG-Token model outperformed the RAG-Sequence model. The authors propose that this improvement in performance is because the answers to Jeopardy questions may be split over several documents. Finally, human assessments for the Jeopardy Question Generation task find that RAG responses are both more factually accurate and more specific than BART responses. Overall, the paper’s authors propose a new architecture, which combines a pre-trained retriever model with a pre-trained generation model, which demonstrates performance near or exceeding state of the art models in four different domains. The authors demonstrate that in open-domain QA or abstractive QA, RAG-Sequence models outperform RAG-Token models, while RAG-Token models outperform in scenarios where an answer is spread across multiple sources. They also demonstrate that the information retrieved can be supplemented by the information generated by the generation model’s parametric knowledge, increasing overall performance. Finally, the authors show that humans prefer the responses generated by a RAG architecture over responses generated by a pure generation model. This paper is revolutionary in that it overcomes the knowledge-cutoff date problem present in many early neural language models. By updating the information contained within the dense index, the model will subsequently update its responses, without needing to retrain the generation model. This is a much cheaper option. Additionally, it provides some transparency in terms of where information comes from. Whereas factual knowledge is stored somewhere within the layers of a neural network, information obtained using a retriever has a citation, improving the verifiability of responses. 

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{python}
import pandas as pd
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References

